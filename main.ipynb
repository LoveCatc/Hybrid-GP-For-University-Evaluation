{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose, STL\n",
    "from tqdm import tqdm\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel, RQKernel, RBFKernel, SpectralMixtureKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap some useful functions here\n",
    "\n",
    "def train(model: gpytorch.models, likelihood: gpytorch.likelihoods, training_iters: int, \n",
    "        train_x: torch.tensor, train_y: torch.tensor, lr:float=0.1, patience_iters:int=10, patience:float=1e-3, verbose:bool=False):\n",
    "    \"\"\" Train a model with Adam optimizer and marginal log likelihood.\n",
    "        :param model: \n",
    "            a valid gpytorch GP model (which returns gpytorch.distributions)\n",
    "        :param likelihood:\n",
    "            specifies the mapping from latent function values f to observations y\n",
    "        :param training_iters:\n",
    "            times of training iterations\n",
    "        :param train_x:\n",
    "            inputs for training, with shape (n_samples, n_features)\n",
    "        :param train_y:\n",
    "            (scalar) outputs of train_x, with shape (n_samples, )\n",
    "        :param lr:\n",
    "            learning rate of the optimizer, default 0.1\n",
    "        :param patience_iters:\n",
    "            controls the waiting iters of the loss varaition\n",
    "        :param patience:\n",
    "            controls the variation rate of loss for early stop\n",
    "        :param verbose:\n",
    "            controls whether printing the training info\n",
    "    \"\"\"\n",
    "    # use Adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # use marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    counter = 0\n",
    "    memoryloss = None\n",
    "    if verbose:\n",
    "        with tqdm(total=training_iters) as t:\n",
    "            for i in range(training_iters):\n",
    "                t.set_description(\"Epoch %d, Early-Stop Counter %d\" % (i, counter))\n",
    "                optimizer.zero_grad()\n",
    "                output = model(train_x)\n",
    "                loss = -mll(output, train_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                t.set_postfix(loss=loss.item())\n",
    "                t.update(1)\n",
    "\n",
    "                if i > 0:\n",
    "                    if abs(loss.item() - memoryloss) / abs(memoryloss) < patience:\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        counter = 0\n",
    "\n",
    "                memoryloss = loss.item()\n",
    "\n",
    "                if counter >= patience_iters:\n",
    "                    print(\"Early Stop Triggered!\")\n",
    "                    break\n",
    "    else:\n",
    "        for i in range(training_iters):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i > 0:\n",
    "                if abs(loss.item() - memoryloss) / abs(memoryloss) < patience:\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    counter = 0\n",
    "            memoryloss = loss.item()\n",
    "            if counter >= patience_iters:\n",
    "                break\n",
    "\n",
    "def predict(model: gpytorch.models, likelihood: gpytorch.likelihoods, \n",
    "        test_x: torch.tensor, is_observed:bool=True):\n",
    "    \"\"\" Use a trained model to make predictions.\n",
    "        :param model: \n",
    "            a valid gpytorch GP model (which returns gpytorch.distributions)\n",
    "        :param likelihood:\n",
    "            specifies the mapping from latent function values f to observations y\n",
    "        :param test_x:\n",
    "            inputs to predict, with shape (n_samples, n_features)\n",
    "        :param is_observed:\n",
    "            whether the outputs should be observed or latent outputs\n",
    "        :return:\n",
    "            outputs at the input points\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        if is_observed:\n",
    "            return likelihood(model(test_x))\n",
    "        else:\n",
    "            return model(test_x)\n",
    "\n",
    "def plot(predictions: gpytorch.distributions, train_x: torch.tensor,\n",
    "        train_y: torch.tensor, test_x: torch.tensor):\n",
    "    \"\"\" Plot the predicted results (2-D only).\n",
    "        :param predictions:\n",
    "            gpytorch.distributions, usually MultivariateNormal\n",
    "        :param train_x:\n",
    "            the input points of training, with shape (n_samples, )\n",
    "        :param train_y:\n",
    "            the output points of training, with shape (n_samples, )\n",
    "        :param test_x:\n",
    "            the input points to predict, with shape (n_samples, )\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        f, ax = plt.subplots(1, 1, figsize=(4,3))\n",
    "        # confidence bounds\n",
    "        lower, upper = predictions.confidence_region()\n",
    "        # training data is plotted as black stars\n",
    "        ax.plot(train_x.cpu().numpy(), train_y.cpu().numpy(), \"k*\")\n",
    "        # predicted mean is plotted as blue line\n",
    "        ax.plot(test_x.cpu().numpy(), predictions.mean.cpu().numpy(), \"b\")\n",
    "        # shade the confidence region\n",
    "        ax.fill_between(test_x.cpu().numpy(), lower.cpu().numpy(), upper.cpu().numpy(), alpha=0.5)\n",
    "        # legend & x/ylim\n",
    "        # ax.set_ylim([-3,3])\n",
    "        ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized Kernel for Trend Forecasting\n",
    "class NSKernel(gpytorch.kernels.Kernel):\n",
    "    is_stationary = False\n",
    "    has_lengthscale = True\n",
    "\n",
    "    def __init__(self, num_mixtures:int, ard_num_dims:int, nu:float, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.spmk = SpectralMixtureKernel(num_mixtures, ard_num_dims)\n",
    "        self.mk = MaternKernel(nu)\n",
    "        self.k1 = ScaleKernel(self.spmk * self.mk)\n",
    "        self.rbf = RBFKernel()\n",
    "        self.k2 = ScaleKernel(self.rbf)\n",
    "        self.ns_mapper = torch.nn.Sequential(\n",
    "            torch.nn.Linear(ard_num_dims, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 4),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x1, x2, last_dim_is_batch=False):\n",
    "        y1 = self.k1(x1,x2)\n",
    "        y2 = self.k2(self.ns_mapper(x1), self.ns_mapper(x2))\n",
    "        return self.lengthscale * (y1+y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel for Seasonal Component Forecasting\n",
    "# NOTE: may need improvement\n",
    "class PeriodicKernel(gpytorch.kernels.Kernel):\n",
    "    # the sinc kernel is stationary\n",
    "    is_stationary = True\n",
    "\n",
    "    # We will register the parameter when initializing the kernel\n",
    "    def __init__(self, length_prior=None, length_constraint=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # register the raw parameter\n",
    "        self.register_parameter(\n",
    "            name='raw_length', parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, 1))\n",
    "        )\n",
    "\n",
    "        # set the parameter constraint to be positive, when nothing is specified\n",
    "        if length_constraint is None:\n",
    "            length_constraint = gpytorch.constraints.Positive()\n",
    "\n",
    "        # register the constraint\n",
    "        self.register_constraint(\"raw_length\", length_constraint)\n",
    "\n",
    "        # set the parameter prior, see\n",
    "        # https://docs.gpytorch.ai/en/latest/module.html#gpytorch.Module.register_prior\n",
    "        if length_prior is not None:\n",
    "            self.register_prior(\n",
    "                \"length_prior\",\n",
    "                length_prior,\n",
    "                lambda m: m.length,\n",
    "                lambda m, v : m._set_length(v),\n",
    "            )\n",
    "\n",
    "    # now set up the 'actual' paramter\n",
    "    @property\n",
    "    def length(self):\n",
    "        # when accessing the parameter, apply the constraint transform\n",
    "        return self.raw_length_constraint.transform(self.raw_length)\n",
    "\n",
    "    @length.setter\n",
    "    def length(self, value):\n",
    "        return self._set_length(value)\n",
    "\n",
    "    def _set_length(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_length)\n",
    "        # when setting the paramater, transform the actual value to a raw one by applying the inverse transform\n",
    "        self.initialize(raw_length=self.raw_length_constraint.inverse_transform(value))\n",
    "\n",
    "    # this is the kernel function\n",
    "    def forward(self, x1, x2, **params):\n",
    "        # apply lengthscale\n",
    "        x1_ = x1.div(self.length)\n",
    "        x2_ = x2.div(self.length)\n",
    "        # calculate the distance between inputs\n",
    "        diff = self.covar_dist(x1_, x2_, **params)\n",
    "        # prevent divide by 0 errors\n",
    "        diff.where(diff == 0, torch.as_tensor(1e-20).cuda())\n",
    "        # return sinc(diff) = sin(diff) / diff\n",
    "        return torch.sin(diff).div(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Daily-train.csv', 'Hourly-train.csv', 'Monthly-train.csv', 'Quarterly-train.csv', 'Weekly-train.csv', 'Yearly-train.csv']\n",
      "['Daily-test.csv', 'Hourly-test.csv', 'Monthly-test.csv', 'Quarterly-test.csv', 'Weekly-test.csv', 'Yearly-test.csv']\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "# Forecasting Horizons: hourly-48 daily-14 weekly-13 monthly-18 quarterly-8 yearly-6\n",
    "dataset_path = \"./Dataset/\"\n",
    "train_data_path = dataset_path+\"Train/\"\n",
    "test_data_path = dataset_path+\"Test/\"\n",
    "train_datasets = os.listdir(train_data_path)\n",
    "test_datasets = os.listdir(test_data_path)\n",
    "\n",
    "print(train_datasets)\n",
    "print(test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23000, 835)\n",
      "(23000, 6)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(train_data_path + train_datasets[-1],index_col=0)\n",
    "test_data = pd.read_csv(test_data_path + test_datasets[-1],index_col=0)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use torch.utils.data.Dataset as data container\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Tuple\n",
    "\n",
    "class GPTimeserieDataset(Dataset):\n",
    "    \"\"\" Accept the concatenation of training and testing data \n",
    "        in pd.DataFrame (n_series, series_len)\n",
    "        return the available torch.Tensors (TrainData, TestData)\n",
    "\n",
    "        :data_train: the pd.DataFrame (n_series, series_len)\n",
    "        :data_test: forecasting pointrs (n_series, forecast_len)\n",
    "        :window_size: training/testing point for learning, should be forecasting points + 1\n",
    "        :date_freq: frequency between each points of used timeseries \n",
    "        :max_len: if specified, only use series_len smaller than this parameter\n",
    "    \"\"\"\n",
    "    def __init__(self, data_train:pd.DataFrame, data_test:pd.DataFrame,\n",
    "                date_freq:str, window_size:int=None, max_len:int=None):\n",
    "        # pre-process the data\n",
    "        if max_len is not None:\n",
    "            valid_idx = []\n",
    "            for idx,col in data_train.T.items():\n",
    "                if col.dropna().shape[0] <= max_len:\n",
    "                    valid_idx.append(idx)\n",
    "            data_train = data_train.loc[valid_idx, :]\n",
    "            data_test = data_test.loc[valid_idx, :]\n",
    "        else:\n",
    "            data_train = data_train\n",
    "            data_test = data_test\n",
    "\n",
    "        self.data_train = data_train\n",
    "        self.data_test = data_test\n",
    "        self.date_freq = date_freq\n",
    "        if window_size is None:\n",
    "            self.window_size = data_test.shape[-1] + 1\n",
    "        else:\n",
    "            self.window_size = window_size\n",
    "        self.forecast_pts = data_test.shape[-1]\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def _proc_log(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_ = tuple(x[i:i+self.window_size] \\\n",
    "            for i in range(x.shape[0] - self.window_size+1))\n",
    "        x_ = tuple(torch.log(i).reshape(-1,1) for i in x_)\n",
    "        return torch.cat(x_, dim=1)\n",
    "\n",
    "    def _proc(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_ = tuple(x[i:i+self.window_size].reshape(-1,1) \\\n",
    "            for i in range(x.shape[0] - self.window_size+1))\n",
    "        return torch.cat(x_, dim=1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        assert self.data_train.shape[0] == self.data_test.shape[0]\n",
    "        return self.data_train.shape[0]\n",
    "\n",
    "    def __getitem__(self, index) -> \\\n",
    "        Tuple[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor], \\\n",
    "        Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n",
    "        \"\"\" Return ((trend_train_x, trend_train_y), (trend_test_x, trend_test_y), \n",
    "            (seasonal_train_x, seasonal_train_y), (seasonal_test_x, seasonal_test_y), original)\n",
    "        \"\"\"\n",
    "        train = self.data_train.iloc[index, :].dropna()\n",
    "        test = self.data_test.iloc[index, :].dropna()\n",
    "        d = pd.concat((train, test)).values\n",
    "        t = pd.date_range(start=\"2000-01-01\", periods=d.shape[0], freq=self.date_freq)\n",
    "        ts = pd.DataFrame(d).set_index(t)\n",
    "        rslt = seasonal_decompose(ts, model=\"m\", extrapolate_trend=\"freq\")\n",
    "        trends = torch.Tensor(rslt.trend.values).clamp_(1e-6).to(self.device)\n",
    "        seasons = torch.Tensor(rslt.seasonal.values).clamp_(1e-6).to(self.device)\n",
    "        original = torch.Tensor(d).to(self.device)\n",
    "        trends = self._proc_log(trends).T\n",
    "        seasons = self._proc(seasons).T\n",
    "        trends_train_x = trends[:-self.forecast_pts, :-1]\n",
    "        trends_train_y = trends[:-self.forecast_pts, -1]\n",
    "        trends_test_x = trends[-self.forecast_pts:, :-1]\n",
    "        trends_test_y = trends[-self.forecast_pts:, -1]\n",
    "        seasons_train_x = seasons[:-self.forecast_pts, :-1]\n",
    "        seasons_train_y = seasons[:-self.forecast_pts, -1]\n",
    "        seasons_test_x = seasons[-self.forecast_pts:, :-1]\n",
    "        seasons_test_y = seasons[-self.forecast_pts:, -1]\n",
    "        return (\n",
    "            (trends_train_x, trends_train_y), (trends_test_x, trends_test_y),\n",
    "            (seasons_train_x, seasons_train_y), (seasons_test_x, seasons_test_y), original\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarks drawn from github.com/Mcompetitions/M4-methods\n",
    "def smape(a, b):\n",
    "    \"\"\"\n",
    "    Calculates sMAPE\n",
    "    :param a: actual values\n",
    "    :param b: predicted values\n",
    "    :return: sMAPE\n",
    "    \"\"\"\n",
    "    a = np.reshape(a, (-1,))\n",
    "    b = np.reshape(b, (-1,))\n",
    "    return np.mean(2.0 * np.abs(a - b) / (np.abs(a) + np.abs(b))).item()\n",
    "\n",
    "\n",
    "def mase(insample, y_test, y_hat_test, freq):\n",
    "    \"\"\"\n",
    "    Calculates MAsE\n",
    "    :param insample: insample data\n",
    "    :param y_test: out of sample target values\n",
    "    :param y_hat_test: predicted values\n",
    "    :param freq: data frequency\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # this func only works with 1d arrays\n",
    "    insample = insample.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "    y_hat_test = y_hat_test.squeeze()\n",
    "\n",
    "    y_hat_naive = []\n",
    "    for i in range(freq, len(insample)):\n",
    "        y_hat_naive.append(insample[(i - freq)])\n",
    "\n",
    "    masep = np.mean(abs(insample[freq:] - y_hat_naive))\n",
    "\n",
    "    return np.mean(abs(y_test - y_hat_test)) / masep\n",
    "\n",
    "# inverse transformation of trend & seasonal components\n",
    "def ts_trans(trend: torch.Tensor, seasonal: torch.Tensor) -> np.array:\n",
    "    return (torch.exp(trend) * seasonal).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GP\n",
    "class trendGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(trendGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean = gpytorch.means.ConstantMean()\n",
    "        self.cov = NSKernel(4,6,1.5)\n",
    "    def forward(self, x):\n",
    "        mean = self.mean(x)\n",
    "        cov = self.cov(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, cov)\n",
    "\n",
    "class seasonalGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(seasonalGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean = gpytorch.means.ConstantMean()\n",
    "        self.cov = PeriodicKernel()\n",
    "    def forward(self, x):\n",
    "        mean = self.mean(x)\n",
    "        cov = self.cov(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go training\n",
    "from linear_operator.utils.errors import NotPSDError\n",
    "smape_list = []\n",
    "mase_list = []\n",
    "\n",
    "dataloader = DataLoader(GPTimeserieDataset(train_data, test_data, \"1d\"), shuffle=True)\n",
    "counter = 0\n",
    "\n",
    "for ((t_train_x, t_train_y), (t_test_x, t_test_y), \\\n",
    "    (s_train_x, s_train_y), (s_test_x, s_test_y), origin) in tqdm(dataloader):\n",
    "    \n",
    "    counter += 1\n",
    "    if counter >= 1000:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        t_likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()\n",
    "        t_model = trendGP(t_train_x, t_train_y, t_likelihood).cuda()\n",
    "        s_likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()\n",
    "        s_model = seasonalGP(s_train_x, s_train_y, s_likelihood).cuda()\n",
    "        train(t_model, t_likelihood, 1000, t_train_x, t_train_y, lr=0.04, patience=1e-2)\n",
    "        train(s_model, s_likelihood, 1000, s_train_x, s_train_y, lr=0.05)\n",
    "        t_pred_y = predict(t_model, t_likelihood, t_test_x)\n",
    "        s_pred_y = predict(s_model, s_likelihood, s_test_x)\n",
    "        origin = origin.squeeze()\n",
    "        smape_list.append(smape(ts_trans(t_test_y, s_test_y), ts_trans(t_pred_y.mean, s_pred_y.mean)))\n",
    "        mase_list.append(mase(ts_trans(t_train_y, s_train_y), ts_trans(t_test_y, s_test_y),\\\n",
    "            ts_trans(t_pred_y.mean, s_pred_y.mean), freq=1))\n",
    "    except NotPSDError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_mean = np.mean(np.array(smape_list))\n",
    "mase_mean = np.mean(np.array(mase_list))\n",
    "print(smape_mean, mase_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 6, 10)\n",
      "(50, 6, 10)\n",
      "(300, 9) (300, 1)\n"
     ]
    }
   ],
   "source": [
    "# Following part is copied from previous work.\n",
    "# define load data func\n",
    "def load_data_from_csv(filepath: str) -> np.ndarray:\n",
    "    csv_file = pd.read_csv(filepath)\n",
    "    school_num = csv_file.shape[0]\n",
    "    year_num = csv_file.shape[1] - 2    # except first and last column\n",
    "    data_dim = len(csv_file.iloc[0, 1].strip().split())\n",
    "    school_names = tuple(csv_file[\"SchoolName\"])\n",
    "    data = np.zeros((school_num, year_num, data_dim))\n",
    "    for i, _ in enumerate(school_names):\n",
    "        data_row = list(csv_file.iloc[i, 1:year_num+1])\n",
    "        data_row = [_.strip().split() for _ in data_row]\n",
    "        data_row = [float(_) for l in data_row for _ in l]\n",
    "        data_row = np.array(data_row).reshape((year_num, -1))\n",
    "        data[i, :, :] = data_row\n",
    "    return data, school_names\n",
    "\n",
    "data, school_names = load_data_from_csv(\"./Dataset/UE/all-data.csv\")\n",
    "school_n, year_n, data_dim = data.shape\n",
    "idx2sch = {k:v for k,v in enumerate(school_names)}\n",
    "\n",
    "selected_data_idx = np.array([0, 2, 7, 11, 15, 19])\n",
    "selected_data = data[:, :, selected_data_idx]\n",
    "selected_data = selected_data.transpose(0, 2, 1)\n",
    "print(selected_data.shape)\n",
    "\n",
    "more_data_path = \"./Dataset/UE/more-school-all.npy\"\n",
    "more_data = np.load(more_data_path)\n",
    "selected_data = np.concatenate((selected_data, more_data), axis=0)\n",
    "print(selected_data.shape)\n",
    "\n",
    "reshaped = selected_data.reshape(-1, 10)\n",
    "train_data = pd.DataFrame(reshaped[:, :-1])\n",
    "test_data = pd.DataFrame(reshaped[:, -1])\n",
    "\n",
    "train_data[train_data <= 0] = 1e-6\n",
    "test_data[test_data <= 0] = 1e-6\n",
    "\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GP\n",
    "class trendGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(trendGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean = gpytorch.means.ConstantMean()\n",
    "        self.cov = NSKernel(4,1,1.5)\n",
    "    def forward(self, x):\n",
    "        mean = self.mean(x)\n",
    "        cov = self.cov(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, cov)\n",
    "\n",
    "class seasonalGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(seasonalGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean = gpytorch.means.ConstantMean()\n",
    "        self.cov = PeriodicKernel()\n",
    "    def forward(self, x):\n",
    "        mean = self.mean(x)\n",
    "        cov = self.cov(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [16:37<00:00,  3.32s/it]\n"
     ]
    }
   ],
   "source": [
    "# Go training\n",
    "from linear_operator.utils.errors import NotPSDError\n",
    "smape_list = []\n",
    "mase_list = []\n",
    "\n",
    "dataloader = DataLoader(GPTimeserieDataset(train_data, test_data, \"1y\"), shuffle=True)\n",
    "counter = 0\n",
    "\n",
    "for ((t_train_x, t_train_y), (t_test_x, t_test_y), \\\n",
    "    (s_train_x, s_train_y), (s_test_x, s_test_y), origin) in tqdm(dataloader):\n",
    "    \n",
    "    counter += 1\n",
    "    if counter >= 1000:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        t_likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()\n",
    "        t_model = trendGP(t_train_x, t_train_y, t_likelihood).cuda()\n",
    "        s_likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()\n",
    "        s_model = seasonalGP(s_train_x, s_train_y, s_likelihood).cuda()\n",
    "        train(t_model, t_likelihood, 1000, t_train_x, t_train_y, lr=0.04, patience=1e-2)\n",
    "        train(s_model, s_likelihood, 1000, s_train_x, s_train_y, lr=0.05)\n",
    "        t_pred_y = predict(t_model, t_likelihood, t_test_x)\n",
    "        s_pred_y = predict(s_model, s_likelihood, s_test_x)\n",
    "        origin = origin.squeeze()\n",
    "        smape_list.append(smape(ts_trans(t_test_y, s_test_y), ts_trans(t_pred_y.mean, s_pred_y.mean)))\n",
    "        mase_list.append(mase(ts_trans(t_train_y, s_train_y), ts_trans(t_test_y, s_test_y),\\\n",
    "            ts_trans(t_pred_y.mean, s_pred_y.mean), freq=1))\n",
    "    except NotPSDError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_mean = np.mean(np.array(smape_list))\n",
    "mase_mean = np.mean(np.array(mase_list))\n",
    "print(smape_mean, mase_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbc04670318ae53ec6178e62f07668c2540c4d06846c7a34c020425c099473f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
